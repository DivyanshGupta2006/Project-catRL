{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline\n",
    "sns.set_theme()"
   ],
   "id": "74840abded7f043f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.strategy.model import Model\n",
    "from src.strategy.environment import Environment\n",
    "from src.strategy.agent import Agent\n",
    "from src.strategy.buffer import Buffer\n",
    "from src.utils import get_config, read_file\n",
    "config = get_config.read_yaml()"
   ],
   "id": "61681055213a0301",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Starting Training...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "670ba7a1ed5a393e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MODEL_PATH = config['paths']['model_directory']\n",
    "CAPITAL = config['strategy']['capital']\n",
    "SYMBOLS = config['data']['symbols']\n",
    "\n",
    "hp = config['hyperparameters']\n",
    "NUM_ASSETS = hp['num_assets']\n",
    "INPUT_DIM = hp['input_dim']\n",
    "ACTION_DIM = hp['action_dim']\n",
    "NUM_LSTM_LAYERS = hp['num_lstm_layers']\n",
    "HIDDEN_STATE_DIM = hp['hidden_state_dim']\n",
    "ACTOR_HIDDEN_DIM = hp['actor_hidden_dim']\n",
    "CRITIC_HIDDEN_DIM = hp['critic_hidden_dim']\n",
    "GAMMA = hp['gamma']\n",
    "GAE_LAMBDA = hp['gae_lambda']\n",
    "CLIP_EPSILON = hp['clip_epsilon']\n",
    "VALUE_LOSS_COEF = hp['value_loss_coef']\n",
    "ENTROPY_LOSS_COEF = hp['entropy_loss_coef']\n",
    "LEARNING_RATE = hp['learning_rate']\n",
    "NUM_EPOCHS = hp['num_epochs']\n",
    "ROLLOUT_SIZE = hp['rollout_size']\n",
    "MINI_BATCH_SIZE = hp['mini_batch_size']\n",
    "SEQUENCE_LENGTH = hp['seq_len']\n",
    "train_data_norm = read_file.read_merged_training_data()\n",
    "train_data_unnorm = read_file.read_merged_training_data(False)\n",
    "print(f'MODEL_PATH: {MODEL_PATH}')\n",
    "print(f'CAPITAL: {CAPITAL}')\n",
    "print(f'SYMBOLS: {SYMBOLS}')\n",
    "print(f'NUM_ASSETS: {NUM_ASSETS}')\n",
    "print(f'INPUT_DIM: {INPUT_DIM}')\n",
    "print(f'ACTION_DIM: {ACTION_DIM}')\n",
    "print(f'NUM_LSTM_LAYERS: {NUM_LSTM_LAYERS}')\n",
    "print(f'HIDDEN_STATE_DIM: {HIDDEN_STATE_DIM}')\n",
    "print(f'ACTOR_HIDDEN_DIM: {ACTOR_HIDDEN_DIM}')\n",
    "print(f'CRITIC_HIDDEN_DIM: {CRITIC_HIDDEN_DIM}')\n",
    "print(f'GAMMA: {GAMMA}')\n",
    "print(f'GAE_LAMBDA: {GAE_LAMBDA}')\n",
    "print(f'CLIP_EPSILON: {CLIP_EPSILON}')\n",
    "print(f'VALUE_LOSS_COEF: {VALUE_LOSS_COEF}')\n",
    "print(f'ENTROPY_LOSS_COEF: {ENTROPY_LOSS_COEF}')\n",
    "print(f'LEARNING_RATE: {LEARNING_RATE}')\n",
    "print(f'NUM_EPOCHS: {NUM_EPOCHS}')\n",
    "print(f'ROLLOUT_SIZE: {ROLLOUT_SIZE}')\n",
    "print(f'BATCH_SIZE: {MINI_BATCH_SIZE}')\n",
    "print(f'SEQUENCE_LENGTH: {SEQUENCE_LENGTH}')\n",
    "print(train_data_norm)\n",
    "print(train_data_unnorm)"
   ],
   "id": "5acfd836926d9108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = Model(INPUT_DIM,\n",
    "              HIDDEN_STATE_DIM,\n",
    "              NUM_ASSETS,\n",
    "              NUM_LSTM_LAYERS,\n",
    "              ACTOR_HIDDEN_DIM,\n",
    "              CRITIC_HIDDEN_DIM,)\n",
    "model"
   ],
   "id": "974b929bed04337d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "agent = Agent(model,\n",
    "              GAMMA,\n",
    "              GAE_LAMBDA,\n",
    "              CLIP_EPSILON,\n",
    "              VALUE_LOSS_COEF,\n",
    "              ENTROPY_LOSS_COEF,\n",
    "              LEARNING_RATE,\n",
    "              MINI_BATCH_SIZE,\n",
    "              device,\n",
    "              MODEL_PATH)\n",
    "agent"
   ],
   "id": "1e5014dcf10935d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "env = Environment(train_data_unnorm,\n",
    "                  SEQUENCE_LENGTH,\n",
    "                  NUM_ASSETS,\n",
    "                  SYMBOLS,\n",
    "                  CAPITAL)\n",
    "env"
   ],
   "id": "7f09b3dbf0a96497",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "env.reset()",
   "id": "2e5543e6bba08f12",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "buffer = Buffer(ROLLOUT_SIZE,\n",
    "                SEQUENCE_LENGTH,\n",
    "                INPUT_DIM,\n",
    "                ACTION_DIM,\n",
    "                device)\n",
    "buffer.display()"
   ],
   "id": "d096b91dce8f6e18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_data_norm.iloc[0:SEQUENCE_LENGTH]",
   "id": "f080bc77d4723ab6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "buffer.store_state(train_data_norm.iloc[0:SEQUENCE_LENGTH].values)\n",
    "buffer.states"
   ],
   "id": "11a1210d9cd76fd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for step in tqdm(range(5)):\n",
    "    buffer = agent.get_action_and_value(buffer)\n",
    "    buffer = env.step(buffer.actions[buffer.current_step_action - 1], buffer)\n",
    "# print(buffer.rewards.shape)\n",
    "# print(buffer.values.shape)\n",
    "# print(buffer.dones.shape)\n",
    "buffer = agent.compute_gae(buffer=buffer, next_value=torch.tensor([0.0]), next_done=torch.tensor([0.0]))\n",
    "buffer.actions"
   ],
   "id": "4227554959dfe8f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "buffer.advantages",
   "id": "a2f4f481b213d421",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T20:56:53.593467Z",
     "start_time": "2025-11-29T20:54:39.060225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ============================================================\n",
    "# 1. Simple 1D Line World Environment with n \"rows\"\n",
    "# ============================================================\n",
    "\n",
    "class LineWorldEnv:\n",
    "    \"\"\"\n",
    "    A 1D world with positions 0, 1, ..., n-1.\n",
    "    Start near the middle. Actions: 0=left, 1=right.\n",
    "    Reward: +1 at right end, -1 at left end, 0 otherwise.\n",
    "    Episode ends when you hit an end or when max_steps is reached.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_rows=7, max_steps=20):\n",
    "        self.n_rows = n_rows\n",
    "        self.max_steps = max_steps\n",
    "        self.pos = None\n",
    "        self.steps = None\n",
    "\n",
    "    def reset(self):\n",
    "        # Start in the middle\n",
    "        self.pos = self.n_rows // 2\n",
    "        self.steps = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Encode state as a 1D float in [-1, 1]\n",
    "        # position / (n_rows-1) -> [0,1]; then scale to [-1,1]\n",
    "        x = self.pos / (self.n_rows - 1)\n",
    "        x = 2.0 * x - 1.0\n",
    "        return np.array([x], dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        # action: 0 = left, 1 = right\n",
    "        if action == 0:\n",
    "            self.pos -= 1\n",
    "        elif action == 1:\n",
    "            self.pos += 1\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        done = False\n",
    "        reward = 0.0\n",
    "\n",
    "        # Bounds and terminal conditions\n",
    "        if self.pos <= 0:\n",
    "            self.pos = 0\n",
    "            reward = -1.0\n",
    "            done = True\n",
    "        elif self.pos >= self.n_rows - 1:\n",
    "            self.pos = self.n_rows - 1\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "        elif self.steps >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    @property\n",
    "    def obs_dim(self):\n",
    "        return 1  # single scalar\n",
    "\n",
    "    @property\n",
    "    def act_dim(self):\n",
    "        return 2  # left or right\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Actor-Critic Network (Policy πθ and Value Vφ)\n",
    "# ============================================================\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared body -> separate policy head and value head.\n",
    "    For discrete actions, policy head outputs logits for a Categorical dist.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes=(64, 64)):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        last_dim = obs_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(last_dim, h))\n",
    "            layers.append(nn.Tanh())\n",
    "            last_dim = h\n",
    "        self.shared = nn.Sequential(*layers)\n",
    "\n",
    "        # Policy head\n",
    "        self.policy_head = nn.Linear(last_dim, act_dim)\n",
    "\n",
    "        # Value head\n",
    "        self.value_head = nn.Linear(last_dim, 1)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # obs: [batch_size, obs_dim]\n",
    "        x = self.shared(obs)\n",
    "        logits = self.policy_head(x)          # for πθ(a|s)\n",
    "        value = self.value_head(x).squeeze(-1)  # for Vφ(s), shape [batch_size]\n",
    "        return logits, value\n",
    "\n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        \"\"\"\n",
    "        obs: tensor [batch_size, obs_dim]\n",
    "        action (optional): tensor [batch_size] of actions.\n",
    "        Returns:\n",
    "            action: sampled if not provided\n",
    "            logprob: log πθ(a|s)\n",
    "            entropy: H[πθ(·|s)]\n",
    "            value: Vφ(s)\n",
    "        \"\"\"\n",
    "        logits, value = self.forward(obs)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "\n",
    "        if action is None:\n",
    "            action = dist.sample()\n",
    "\n",
    "        logprob = dist.log_prob(action)\n",
    "        entropy = dist.entropy()\n",
    "        return action, logprob, entropy, value\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. GAE-Lambda Advantage and Return Computation\n",
    "# ============================================================\n",
    "\n",
    "def compute_gae(rewards, values, dones, gamma, lam):\n",
    "    \"\"\"\n",
    "    rewards: [T] tensor\n",
    "    values:  [T+1] tensor (note the extra last value for bootstrap)\n",
    "    dones:   [T] tensor of 0/1\n",
    "    Returns:\n",
    "        advantages [T]\n",
    "        returns    [T] = advantages + values[:-1]\n",
    "    \"\"\"\n",
    "    T = len(rewards)\n",
    "    advantages = torch.zeros(T, dtype=torch.float32)\n",
    "    gae = 0.0\n",
    "\n",
    "    # Work backwards from last step to first\n",
    "    for t in reversed(range(T)):\n",
    "        # δ_t = r_t + γ (1-d_t) V(s_{t+1}) - V(s_t)\n",
    "        delta = rewards[t] + gamma * (1 - dones[t]) * values[t+1] - values[t]\n",
    "        # A_t = δ_t + γ λ (1-d_t) A_{t+1}\n",
    "        gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
    "        advantages[t] = gae\n",
    "\n",
    "    returns = advantages + values[:-1]\n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Main PPO Training Loop\n",
    "# ============================================================\n",
    "\n",
    "def ppo_train(\n",
    "    n_rows=7,\n",
    "    total_iterations=500,\n",
    "    steps_per_iter=256,\n",
    "    gamma=0.99,\n",
    "    lam=0.95,\n",
    "    clip_eps=0.2,\n",
    "    lr=3e-4,\n",
    "    train_epochs=4,\n",
    "    minibatch_size=64,\n",
    "    value_coef=0.5,\n",
    "    entropy_coef=0.01,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    env = LineWorldEnv(n_rows=n_rows)\n",
    "    obs_dim = env.obs_dim\n",
    "    act_dim = env.act_dim\n",
    "\n",
    "    model = ActorCritic(obs_dim, act_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for it in range(total_iterations):\n",
    "        # ------------------------------------------\n",
    "        # 4.1 Collect a batch of experience\n",
    "        # ------------------------------------------\n",
    "        obs_buf = []\n",
    "        act_buf = []\n",
    "        rew_buf = []\n",
    "        done_buf = []\n",
    "        logprob_buf = []\n",
    "        val_buf = []\n",
    "\n",
    "        obs = env.reset()\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=device)\n",
    "\n",
    "        for step in range(steps_per_iter):\n",
    "            with torch.no_grad():\n",
    "                # π_{θ_old}(·|s_t) and Vφ(s_t)\n",
    "                action, logprob, _, value = model.get_action_and_value(obs.unsqueeze(0))\n",
    "            action = action.item()\n",
    "            logprob = logprob.item()\n",
    "            value = value.item()\n",
    "\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            next_obs_t = torch.tensor(next_obs, dtype=torch.float32, device=device)\n",
    "\n",
    "            # Store transition\n",
    "            obs_buf.append(obs.cpu().numpy())\n",
    "            act_buf.append(action)\n",
    "            rew_buf.append(reward)\n",
    "            done_buf.append(float(done))\n",
    "            logprob_buf.append(logprob)\n",
    "            val_buf.append(value)\n",
    "\n",
    "            obs = next_obs_t\n",
    "            if done:\n",
    "                obs = torch.tensor(env.reset(), dtype=torch.float32, device=device)\n",
    "\n",
    "        # At this point, we have steps_per_iter transitions.\n",
    "        # We need one extra value for V(s_T) for bootstrapping.\n",
    "        with torch.no_grad():\n",
    "            _, _, _, last_value = model.get_action_and_value(obs.unsqueeze(0))\n",
    "        last_value = last_value.item()\n",
    "\n",
    "        # Convert buffers to tensors\n",
    "        obs_tensor = torch.tensor(np.array(obs_buf), dtype=torch.float32, device=device)\n",
    "        act_tensor = torch.tensor(act_buf, dtype=torch.long, device=device)\n",
    "        rew_tensor = torch.tensor(rew_buf, dtype=torch.float32, device=device)\n",
    "        done_tensor = torch.tensor(done_buf, dtype=torch.float32, device=device)\n",
    "        val_tensor = torch.tensor(val_buf + [last_value], dtype=torch.float32, device=device)\n",
    "\n",
    "        # ------------------------------------------\n",
    "        # 4.2 Compute advantages and returns (GAE)\n",
    "        # ------------------------------------------\n",
    "        advantages, returns = compute_gae(\n",
    "            rewards=rew_tensor,\n",
    "            values=val_tensor,\n",
    "            dones=done_tensor,\n",
    "            gamma=gamma,\n",
    "            lam=lam,\n",
    "        )\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Old logprobs as tensor\n",
    "        old_logprob_tensor = torch.tensor(logprob_buf, dtype=torch.float32, device=device)\n",
    "\n",
    "        # ------------------------------------------\n",
    "        # 4.3 PPO Updates (multiple epochs over batch)\n",
    "        # ------------------------------------------\n",
    "        batch_size = steps_per_iter\n",
    "        indices = np.arange(batch_size)\n",
    "\n",
    "        for epoch in range(train_epochs):\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            for start in range(0, batch_size, minibatch_size):\n",
    "                end = start + minibatch_size\n",
    "                mb_inds = indices[start:end]\n",
    "\n",
    "                mb_obs = obs_tensor[mb_inds]\n",
    "                mb_act = act_tensor[mb_inds]\n",
    "                mb_adv = advantages[mb_inds]\n",
    "                mb_ret = returns[mb_inds]\n",
    "                mb_logprob_old = old_logprob_tensor[mb_inds]\n",
    "\n",
    "                # Forward pass under current θ,ϕ\n",
    "                _, mb_logprob, mb_entropy, mb_value = model.get_action_and_value(mb_obs, mb_act)\n",
    "\n",
    "                # r_t(θ) = exp(logπθ - logπθ_old)\n",
    "                ratio = torch.exp(mb_logprob - mb_logprob_old)\n",
    "\n",
    "                # Clipped surrogate objective:\n",
    "                # L^{CLIP}(θ) = E[min( r_t A_t , clip(r_t,1-ε,1+ε) A_t )]\n",
    "                unclipped = ratio * mb_adv\n",
    "                clipped = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * mb_adv\n",
    "                policy_loss = -torch.mean(torch.min(unclipped, clipped))\n",
    "\n",
    "                # Value function loss: (Vφ(s) - Ĝ_t)^2\n",
    "                value_loss = torch.mean((mb_value - mb_ret) ** 2)\n",
    "\n",
    "                # Entropy bonus (we subtract in loss => encourages exploration)\n",
    "                entropy_loss = -torch.mean(mb_entropy)\n",
    "\n",
    "                # Total loss\n",
    "                loss = policy_loss + value_coef * value_loss + entropy_coef * entropy_loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # ------------------------------------------\n",
    "        # 4.4 Logging (optional, simple print)\n",
    "        # ------------------------------------------\n",
    "        # Rough evaluation: run one episode with greedy policy\n",
    "        if (it + 1) % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                obs_eval = torch.tensor(env.reset(), dtype=torch.float32, device=device)\n",
    "                done_eval = False\n",
    "                ep_return = 0.0\n",
    "                while not done_eval:\n",
    "                    logits, _ = model.forward(obs_eval.unsqueeze(0))\n",
    "                    action = torch.argmax(logits, dim=-1).item()\n",
    "                    next_obs_eval, r_eval, done_eval, _ = env.step(action)\n",
    "                    ep_return += r_eval\n",
    "                    obs_eval = torch.tensor(next_obs_eval, dtype=torch.float32, device=device)\n",
    "            print(f\"Iteration {it+1}: example episode return = {ep_return:.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ppo_train()"
   ],
   "id": "3ee6a504e5bc3b98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50: example episode return = 1.00\n",
      "Iteration 100: example episode return = 1.00\n",
      "Iteration 150: example episode return = 1.00\n",
      "Iteration 200: example episode return = 1.00\n",
      "Iteration 250: example episode return = 1.00\n",
      "Iteration 300: example episode return = 1.00\n",
      "Iteration 350: example episode return = 1.00\n",
      "Iteration 400: example episode return = 1.00\n",
      "Iteration 450: example episode return = 1.00\n",
      "Iteration 500: example episode return = 1.00\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "564779c4583be300"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
